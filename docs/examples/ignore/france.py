# type: ignore
import mellea
from mellea.backends.aloras.huggingface.granite_aloras import add_granite_aloras
from mellea.backends.model_ids import IBM_GRANITE_3_2_8B
from mellea.stdlib.base import Context, SimpleContext

# Let's create a huggingface backend that can generate up to 2048 tokens and has the alora backends activated.
m = mellea.start_session(
    backend_name="hf",
    model_id=IBM_GRANITE_3_2_8B,
    ctx=SimpleContext(),
    model_options={"max_new_tokens": 2048},
)
m.load_default_aloras()


def check_format(ctx: Context):
    c = ctx.last_output()
    if c is None:
        return False
    try:
        float(c.value)
        return True
    except:  # noqa: E722
        return False


def get_french_gdps(sy=1990, ey=2000):
    requirements = [
        m.req(
            "Answer using the format NNNN.NN. Do not include any other words.",
            validation_fn=check_format,
        ),
        m.req("Answer in billions"),
    ]
    france_gdps = {}
    for year in range(sy, ey):
        answer = m.instruct(
            f"Provide the GDP of France in {year}", requirements=requirements
        )
        print(answer)
        validation_results = m.validate(
            requirements, return_full_validation_results=True
        )
        print(validation_results)
        france_gdps[str(year)] = (
            answer.value.split(" ")[0]
            if all(b for [_, b] in validation_results)
            else None
        )
    return france_gdps


print(get_french_gdps())


m.reset()
result = m.chat(
    "List the French GDP from 1990 to 2000. List the amounts in billions of USD."
)
print(result)


# Both of these give different wrong answers. We could continue this example by using a sampling strategy until these two prompting strateiges align. or something like that.
